{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `production_2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv ../src/.env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': True})\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Date       Open       High        Low      Close  \\\n",
      "ticker                                                                         \n",
      "A      2013-12-02 00:00:00-05:00  35.052676  35.183785  34.816674  34.882229   \n",
      "A      2013-12-03 00:00:00-05:00  34.692137  34.856029  34.456136  34.698692   \n",
      "A      2013-12-04 00:00:00-05:00  34.639663  35.288666  34.560998  35.124775   \n",
      "A      2013-12-05 00:00:00-05:00  34.974005  35.269005  34.823227  35.072338   \n",
      "A      2013-12-06 00:00:00-05:00  35.242792  36.009791  35.242792  35.944237   \n",
      "\n",
      "         Volume  Dividends  Stock Splits  year  \n",
      "ticker                                          \n",
      "A       2039962        0.0           0.0  2013  \n",
      "A       3462706        0.0           0.0  2013  \n",
      "A       3377288        0.0           0.0  2013  \n",
      "A       2530939        0.0           0.0  2013  \n",
      "A       4268513        0.0           0.0  2013  \n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Write your code below.\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"*/*.parquet\"))\n",
    "\n",
    "# Check if parquet_files is empty\n",
    "if not parquet_files:\n",
    "    print(\"No Parquet files found in the specified directory.\")\n",
    "else:\n",
    "    # Read Parquet files into Dask DataFrame\n",
    "    dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")\n",
    "    print(dd_px.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Adjusted Close:\n",
    "    \n",
    "    - `returns`: (Adj Close / Adj Close_lag) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1m/2rr7b3011gg02bt4wxfsr3hc0000gn/T/ipykernel_1239/1955404551.py:4: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_rets = (dd_px.groupby('ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "# import numpy as np\n",
    "\n",
    "dd_rets = (dd_px.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(Close_lag_1 = x['Close'].shift(1))\n",
    ").assign(\n",
    "    returns = lambda x: x['Close']/x['Close_lag_1'] - 1\n",
    ").assign(\n",
    "    positive_return = lambda x: (x['returns'] > 0)*1\n",
    "))\n",
    "# def process_data(file):\n",
    "#     # Read parquet file into Dask DataFrame\n",
    "#     df = dd.read_parquet(file)\n",
    "    \n",
    "#     # Add lags for Close and Adj_Close\n",
    "#     df['Close_lag'] = df['Close'].shift(1)\n",
    "#     df['Adj_Close_lag'] = df['Adj_Close'].shift(1)\n",
    "    \n",
    "#     # Add returns\n",
    "#     df['returns'] = (df['Adj_Close'] / df['Adj_Close_lag']) - 1\n",
    "    \n",
    "#     # Add hi_lo_range\n",
    "#     df['hi_lo_range'] = df['High'] - df['Low']\n",
    "    \n",
    "#     return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a rolling average return calculation with a window of 10 days.\n",
    "+ *Tip*: Consider using `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Date       Open       High        Low      Close  \\\n",
      "ticker                                                                         \n",
      "A      2013-12-02 00:00:00-05:00  35.052676  35.183785  34.816674  34.882229   \n",
      "A      2013-12-03 00:00:00-05:00  34.692137  34.856029  34.456136  34.698692   \n",
      "A      2013-12-04 00:00:00-05:00  34.639663  35.288666  34.560998  35.124775   \n",
      "A      2013-12-05 00:00:00-05:00  34.974005  35.269005  34.823227  35.072338   \n",
      "A      2013-12-06 00:00:00-05:00  35.242792  36.009791  35.242792  35.944237   \n",
      "\n",
      "         Volume  Dividends  Stock Splits  year  Close_lag_1   returns  \\\n",
      "ticker                                                                  \n",
      "A       2039962        0.0           0.0  2013          NaN       NaN   \n",
      "A       3462706        0.0           0.0  2013    34.882229 -0.005262   \n",
      "A       3377288        0.0           0.0  2013    34.698692  0.012280   \n",
      "A       2530939        0.0           0.0  2013    35.124775 -0.001493   \n",
      "A       4268513        0.0           0.0  2013    35.072338  0.024860   \n",
      "\n",
      "        positive_return  rolling_avg_return  \n",
      "ticker                                       \n",
      "A                     0                 NaN  \n",
      "A                     0                 NaN  \n",
      "A                     1                 NaN  \n",
      "A                     0                 NaN  \n",
      "A                     1                 NaN  \n"
     ]
    }
   ],
   "source": [
    "# Write your code below.\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the Dask DataFrame to a Pandas DataFrame\n",
    "dd_rets_pd = dd_rets.compute()\n",
    "\n",
    "# Add a rolling average return calculation with a window of 10 days\n",
    "dd_rets_pd['rolling_avg_return'] = dd_rets_pd['returns'].rolling(window=10).mean()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(dd_rets_pd.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "No, it wasn't strictly necessary to convert the Dask DataFrame to a Pandas DataFrame to calculate the rolling average return. It's often better to perform computations directly in Dask when dealing with large datasets because Dask is designed to handle parallel computation and distributed processing efficiently, especially when working with data that doesn't fit into memory.\n",
    "\n",
    "Calculating the rolling average return in Dask would likely have been more efficient, especially if the dataset is large, as Dask can handle the computation in a distributed manner, potentially utilizing multiple cores or even distributed computing clusters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
